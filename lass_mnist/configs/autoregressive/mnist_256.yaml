_target_: transformers.GPT2LMHeadModel
config:
  _target_: transformers.GPT2Config
  vocab_size: 256
  n_positions: 49
  n_embd: 128
  n_layer: 3
  n_head: 2
  use_cache: false
  bos_token_id: 0
  eos_token_id: 511