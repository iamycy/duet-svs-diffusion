# @package _global_

sampling_rate: 48000
length: 262144
channels: 1
log_every_n_steps: 1000

encoder_max_length: 512
encoder_features: 768

model:
  _target_: main.module_diff_textcond.Model
  lr: 1e-4
  lr_beta1: 0.95
  lr_beta2: 0.999
  lr_eps: 1e-6
  lr_weight_decay: 1e-3
  ema_beta: 0.9999
  ema_power: 0.7

  model:
    _target_: audio_diffusion_pytorch.AudioDiffusionConditional
    in_channels: ${channels}
    channels: 160
    patch_size: 16
    resnet_groups: 8
    kernel_multiplier_downsample: 2
    multipliers: [1, 2, 4, 4, 4, 4, 4]
    factors: [4, 2, 2, 2, 2, 2]
    num_blocks: [2, 2, 2, 2, 2, 2]
    attentions: [0, 0, 0, 1, 1, 1, 1]
    attention_heads: 8
    attention_features: 64
    attention_multiplier: 2
    use_nearest_upsample: False
    use_skip_scale: True
    diffusion_sigma_distribution:
      _target_: audio_diffusion_pytorch.UniformDistribution
    embedding_max_length: ${encoder_max_length}
    embedding_features: ${encoder_features}
    embedding_mask_proba: 0.1

  tokenizer: t5-base
  encoder_num_tokens: 32128 # T5 vocab size
  encoder_features: ${encoder_features}
  encoder_max_length: ${encoder_max_length}

  encoder:
    _target_: a_transformers_pytorch.transformers.Transformer
    features: 768
    max_length: 512
    num_layers: 8
    head_features: 64
    num_heads: 8
    multiplier: 4


batch_size: 4

datamodule:
  _target_: main.module_diff_textcond.Datamodule

  dataset_train:
    _target_: audio_data_pytorch.ClothoDataset
    root: ${data_dir}
    split: train
    batch_size: ${batch_size}
    preprocess_sample_rate: 48000
    preprocess_transforms:
      _target_: audio_data_pytorch.AllTransform
      crop_size: 480000
      stereo: True
    transforms:
      _target_: audio_data_pytorch.AllTransform
      target_rate: ${sampling_rate}
      random_crop_size: ${length}
      mono: True
      loudness: -20

  dataset_valid:
    _target_: audio_data_pytorch.ClothoDataset
    root: ${data_dir}
    split: valid
    batch_size: ${batch_size}
    preprocess_sample_rate: 48000
    preprocess_transforms:
      _target_: audio_data_pytorch.AllTransform
      crop_size: 480000
      stereo: True
    transforms:
      _target_: audio_data_pytorch.AllTransform
      target_rate: ${sampling_rate}
      random_crop_size: ${length}
      mono: True
      loudness: -20

  num_workers: 8
  pin_memory: True

callbacks:
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "valid_loss"   # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: True         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    verbose: False
    dirpath: ${logs_dir}/ckpts/${now:%Y-%m-%d-%H-%M-%S}
    filename: '{epoch:02d}-{valid_loss:.3f}'

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 2

  audio_samples_logger:
    _target_: main.module_diff_textcond.SampleLogger
    num_items: 3
    channels: ${channels}
    sampling_rate: ${sampling_rate}
    length: ${length}
    sampling_steps: [3]
    embedding_scale: 7.0
    use_ema_model: False
    diffusion_sampler:
      _target_: audio_diffusion_pytorch.VSampler
    diffusion_schedule:
      _target_: audio_diffusion_pytorch.LinearSchedule

loggers:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: ${oc.env:WANDB_PROJECT}
    entity: ${oc.env:WANDB_ENTITY}
    # offline: False  # set True to store all logs only locally
    job_type: "train"
    group: ""
    save_dir: ${logs_dir}

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0 # Set `1` to train on GPU, `0` to train on CPU only, and `-1` to train on all GPUs, default `0`
  precision: 32 # Precision used for tensors, default `32`
  accelerator: null # `ddp` GPUs train individually and sync gradients, default `None`
  min_epochs: 0
  max_epochs: -1
  enable_model_summary: False
  log_every_n_steps: 1 # Logs metrics every N batches
  check_val_every_n_epoch: null
  limit_val_batches: 20
  val_check_interval: ${log_every_n_steps}
